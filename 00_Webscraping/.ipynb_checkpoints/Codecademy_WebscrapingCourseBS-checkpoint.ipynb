{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c022f35",
   "metadata": {},
   "source": [
    "# Wescraping BS Codecademy Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c60777",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1bbf4e",
   "metadata": {},
   "source": [
    "Before we get started, a quick note on prerequisites: This course requires knowledge of Python. Also some understanding of the Python library Pandas will be helpful later on in the lesson, but isn’t totally necessary. If you haven’t already, check out those courses before taking this one. Okay, let’s get scraping!\n",
    "\n",
    "In Data Science, we can do a lot of exciting work with the right dataset. Once we have interesting data, we can use Pandas or Matplotlib to analyze or visualize trends. But how do we get that data in the first place?\n",
    "\n",
    "If it’s provided to us in a well-organized csv or json file, we’re lucky! Most of the time, we need to go out and search for it ourselves.\n",
    "\n",
    "Often times you’ll find the perfect website that has all the data you need, but there’s no way to download it. This is where BeautifulSoup comes in handy to scrape the HTML. If we find the data we want to analyze online, we can use BeautifulSoup to grab it and turn it into a structure we can understand. This Python library, which takes its name from a song in Alice in Wonderland, allows us to easily and quickly take information from a website and put it into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84942849",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76657f66",
   "metadata": {},
   "source": [
    "1.We’ve used BeautifulSoup to take the turtle data from the Shellter website in the browser and put it into a DataFrame.\n",
    "\n",
    "Explore the website a bit. Then, print the DataFrame turtles to see how this data is organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12811a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import turtles\n",
    "print(turtles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683f16e",
   "metadata": {},
   "source": [
    "                            0  ...                             4\n",
    "Aesop        AGE: 7 Years Old  ...    SOURCE: found in Lake Erie\n",
    "Caesar       AGE: 2 Years Old  ...      SOURCE: hatched in house\n",
    "Sulla         AGE: 1 Year Old  ...    SOURCE: found in Lake Erie\n",
    "Spyro        AGE: 6 Years Old  ...      SOURCE: hatched in house\n",
    "Zelda        AGE: 3 Years Old  ...  SOURCE: surrendered by owner\n",
    "Bandicoot    AGE: 2 Years Old  ...      SOURCE: hatched in house\n",
    "Hal           AGE: 1 Year Old  ...  SOURCE: surrendered by owner\n",
    "Mock        AGE: 10 Years Old  ...  SOURCE: surrendered by owner\n",
    "Sparrow    AGE: 1.5 Years Old  ...    SOURCE: found in Lake Erie\n",
    "\n",
    "[9 rows x 5 columns]\n",
    " \n",
    "1/11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae437878",
   "metadata": {},
   "source": [
    "## Rules of Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89085048",
   "metadata": {},
   "source": [
    "When we scrape websites, we have to make sure we are following some guidelines so that we are treating the websites and their owners with respect.\n",
    "\n",
    "Always check a website’s Terms and Conditions before scraping. Read the statement on the legal use of data. Usually, the data you scrape should not be used for commercial purposes.\n",
    "\n",
    "Do not spam the website with a ton of requests. A large number of requests can break a website that is unprepared for that level of traffic. As a general rule of good practice, make one request to one webpage per second.\n",
    "\n",
    "If the layout of the website changes, you will have to change your scraping code to follow the new structure of the site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058cc81",
   "metadata": {},
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23120039",
   "metadata": {},
   "source": [
    "In order to get the HTML of the website, we need to make a request to get the content of the webpage. To learn more about requests in a general sense, you can check out this https://www.codecademy.com/articles/http-requests.\n",
    "\n",
    "Python has a *requests* library that makes getting content really easy. All we have to do is import the library, and then feed in the URL we want to *GET*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "webpage = requests.get('https://www.codecademy.com/articles/http-requests')\n",
    "print(webpage.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761ceb2",
   "metadata": {},
   "source": [
    "This code will print out the HTML of the page.\n",
    "\n",
    "We don’t want to unleash a bunch of requests on any one website in this lesson, so for the rest of this lesson we will be scraping a local HTML file and pretending it’s an HTML file hosted online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07692ef9",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701dfe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.\n",
    "Import the requests library.\n",
    "\n",
    "\n",
    "2.\n",
    "Make a GET request to the URL containing the turtle adoption website:\n",
    "\n",
    "https://content.codecademy.com/courses/beautifulsoup/shellter.html\n",
    "\n",
    "Store the result of your request in a variable called webpage_response.\n",
    "\n",
    "\n",
    "3.\n",
    "Store the content of the response in a variable called webpage by using .content.\n",
    "\n",
    "Print webpage out to see the content of this HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce741f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #import the requests library\n",
    "webpage_response = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html') #make a get request\n",
    "webpage = webpage_response.content #store the content of the HTML\n",
    "print(webpage) #print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70479e05",
   "metadata": {},
   "source": [
    "## The BeautifulSoup Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532670d8",
   "metadata": {},
   "source": [
    "When we printed out all of that HTML from our request, it seemed pretty long and messy. How could we pull out the relevant information from that long string?\n",
    "\n",
    "BeautifulSoup is a Python library that makes it easy for us to traverse an HTML page and pull out the parts we’re interested in. We can import it by using the line:\n",
    "\n",
    "*from bs4 import BeautifulSoup*\n",
    "\n",
    "Then, all we have to do is convert the HTML document to a BeautifulSoup object!\n",
    "\n",
    "If this is our HTML file, rainbow.html:\n",
    "\n",
    "<body>\n",
    "  <div>red</div>\n",
    "  <div>orange</div>\n",
    "  <div>yellow</div>\n",
    "  <div>green</div>\n",
    "  <div>blue</div>\n",
    "  <div>indigo</div>\n",
    "  <div>violet</div>\n",
    "</body>\n",
    "\n",
    "*soup = BeautifulSoup(\"rainbow.html\", \"html.parser\")*\n",
    "\n",
    "\"html.parser\" is one option for parsers we could use. There are other options, like \"lxml\" and \"html5lib\" that have different advantages and disadvantages, but for our purposes we will be using \"html.parser\" throughout.\n",
    "\n",
    "With the requests skills we just learned, we can use a website hosted online as that HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98998089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "webpage = requests.get(\"http://rainbow.com/rainbow.html\", \"html.parser\")\n",
    "soup = bs(webpage.content)\n",
    "\n",
    "#When we use BeautifulSoup in combination with pandas, we can turn websites into DataFrames \n",
    "#that are easy to manipulate and gain insights from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d58141",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162b5bd",
   "metadata": {},
   "source": [
    "1.\n",
    "Import the BeautifulSoup package.\n",
    "\n",
    "\n",
    "2.\n",
    "Create a BeautifulSoup object out of the webpage content and call it soup. Use \"html.parser\" as the parser.\n",
    "\n",
    "Print out soup! Look at how it contains all of the HTML of the page! We will learn how to traverse this content and find what we need in the next exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import the needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then we use *get* from *requests* to pull the website html content in *wepage_response*\n",
    "webpage_response = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "#we store that content in the variable webpage\n",
    "webpage = webpage_response.content\n",
    "#then we create a soup object to store the parsed html of the page\n",
    "soup = bs(webpage,'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7382a73",
   "metadata": {},
   "source": [
    "## Object Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc5d51",
   "metadata": {},
   "source": [
    "BeautifulSoup breaks the HTML page into several types of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf8e7d",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642198b",
   "metadata": {},
   "source": [
    "A Tag corresponds to an HTML Tag in the original document. These lines of code:\n",
    "\n",
    "soup = BeautifulSoup('<div id=\"example\">An example div</div><p>An example p tag</p>')\n",
    "print(soup.div)\n",
    "\n",
    "Would produce output that looks like:\n",
    "\n",
    "<div id=\"example\">An example div</div>\n",
    "\n",
    "Accessing a tag from the BeautifulSoup object in this way will get the first tag of that type on the page.\n",
    "\n",
    "You can get the name of the tag using .name and a dictionary representing the attributes of the tag using .attrs:\n",
    "\n",
    "print(soup.div.name)\n",
    "print(soup.div.attrs)\n",
    "\n",
    "div\n",
    "{'id': 'example'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we try the code explained in the previous paragraph\n",
    "soup = bs('<div id=\"example\">An example div</div><p>An example p tag</p>')\n",
    "print(soup.div)\n",
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a451b80",
   "metadata": {},
   "source": [
    "Accessing a tag from the BeautifulSoup object in this way will get the first tag of that type on the page.\n",
    "\n",
    "You can get the name of the tag using .name and a dictionary representing the attributes of the tag using .attrs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.div.name) #div\n",
    "print(soup.div.attrs) #{'id': 'example'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a559a",
   "metadata": {},
   "source": [
    "## NavigableStrings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcee090",
   "metadata": {},
   "source": [
    "NavigableStrings are the pieces of text that are in the HTML tags on the page. You can get the string inside of the tag by calling .string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.div.string) #An example div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c999ee5",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1.\n",
    "Print out the first p tag on the shellter.html page.\n",
    "\n",
    "2.\n",
    "Print out the string associated with the first p tag on the shellter.html page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d90b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import the needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then we request to access a webpage content, and save it\n",
    "webpage_response = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "webpage = webpage_response.content\n",
    "\n",
    "#then we create the bs object\n",
    "soup = bs(webpage, 'html.parser')\n",
    "\n",
    "#now we solve exercise 1\n",
    "print(soup.p) #output --> <p class=\"text\">Click to learn more about each turtle</p>\n",
    "\n",
    "#and we solve exercise 2\n",
    "print(soup.p.string) #output --> Click to learn more about each turtle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b728919",
   "metadata": {},
   "source": [
    "#### UN INCISO EN EL CURSO, PARA VER LAS DIFERENTES VISUALIZACIONES DE UN HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the webpage using parser\n",
    "soup = bs(webpage, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cad06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the website without parser *barely any difference*\n",
    "soup2 =bs(webpage)\n",
    "print(soup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21221d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the website using prettify() organizes the html in hierarchy\n",
    "soup3 =bs(webpage)\n",
    "print(soup3.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8517eb",
   "metadata": {},
   "source": [
    "## Navigatin by Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d968ff",
   "metadata": {},
   "source": [
    "To navigate through a tree, we can call the tag names themselves. Imagine we have an HTML page that looks like this:\n",
    "\n",
    "<h1>World's Best Chocolate Chip Cookies</h1>\n",
    "<div class=\"banner\">\n",
    "  <h1>Ingredients</h1>\n",
    "</div>\n",
    "<ul>\n",
    "  <li> 1 cup flour </li>\n",
    "  <li> 1/2 cup sugar </li>\n",
    "  <li> 2 tbsp oil </li>\n",
    "  <li> 1/2 tsp baking soda </li>\n",
    "  <li> ½ cup chocolate chips </li> \n",
    "  <li> 1/2 tsp vanilla <li>\n",
    "  <li> 2 tbsp milk </li>\n",
    "</ul>\n",
    "\n",
    "If we made a soup object out of this HTML page, we have seen that we can get the first h1 element by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87566c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.h1) #Output --> <h1>World's Best Chocolate Chip Cookies</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can get the children of a tag by accessing the .children attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in soup.ul.children:\n",
    "    print(child)\n",
    "    \n",
    "#Output:\n",
    "#<li> 1 cup flour </li>\n",
    "#<li> 1/2 cup sugar </li>\n",
    "#<li> 2 tbsp oil </li>\n",
    "#<li> 1/2 tsp baking soda </li>\n",
    "#<li> ½ cup chocolate chips </li> \n",
    "#<li> 1/2 tsp vanilla <li>\n",
    "#<li> 2 tbsp milk </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a30db2",
   "metadata": {},
   "source": [
    "We can also navigate up the tree of a tag by accessing the .parents attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parents in soup.li.parents:\n",
    "    print(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32cea0",
   "metadata": {},
   "source": [
    "This loop will first print:\n",
    "\n",
    "<ul>\n",
    "<li> 1 cup flour </li>\n",
    "<li> 1/2 cup sugar </li>\n",
    "<li> 2 tbsp oil </li>\n",
    "<li> 1/2 tsp baking soda </li>\n",
    "<li> ½ cup chocolate chips </li>\n",
    "<li> 1/2 tsp vanilla </li>\n",
    "<li> 2 tbsp milk </li>\n",
    "</ul>\n",
    "\n",
    "Then, it will print the tag that contains the ul (so, the body tag of the document). Then, it will print the tag that contains the body tag (so, the html tag of the document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e3378",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1.\n",
    "Loop through all of the children of the first div and print out each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f394ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import the libraries we need\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then we request the webpage\n",
    "webpage = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "\n",
    "#we save that content in a soup object\n",
    "soup = bs(webpage.content, 'html.parser')\n",
    "\n",
    "#now we do exercise 1, loop through all of the children of the first div and print out each one\n",
    "for child in soup.div.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f046e",
   "metadata": {},
   "source": [
    "## Website Structure \n",
    "\n",
    "When we’re telling our Python script what HTML tags to grab, we need to know the structure of the website and what we’re looking for.\n",
    "\n",
    "Many browsers, including Chrome, Firefox, and Safari, have Dev Tools that help you inspect a webpage and see what HTML elements it is composed of.\n",
    "\n",
    "First, learn how to use DevTools: https://www.codecademy.com/articles/use-devtools\n",
    "\n",
    "Then, when you’re preparing to scrape a website, first inspect the HTML to see where the info you are looking for is located on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d7dd5",
   "metadata": {},
   "source": [
    "## Find all\n",
    "\n",
    "If we want to find all of the occurrences of a tag, instead of just the first one, we can use .find_all().\n",
    "\n",
    "This function can take in just the name of a tag and returns a list of all occurrences of that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da1cd3",
   "metadata": {},
   "source": [
    ".find_all() is far more flexible than just accessing elements directly through the soup object. With .find_all(), we can use regexes, attributes, or even functions to select HTML elements more intelligently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc0852",
   "metadata": {},
   "source": [
    "## Using Regex\n",
    "\n",
    "What if we want every <ol> and every <ul> that the page contains? We can select both of these types of elements with a regex in our .find_all():\n",
    "\n",
    "> import re\n",
    "> soup.find_all(re.compile(\"[ou]l\"))\n",
    "    \n",
    "What if we want all of the h1 - h9 tags that the page contains? Regex to the rescue again!\n",
    "\n",
    "> import re\n",
    "> soup.find_all(re.compile(\"h[1-9]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ebcb4",
   "metadata": {},
   "source": [
    "## Using Lists\n",
    "\n",
    "We can also just specify all of the elements we want to find by supplying the function with a list of the tag names we are looking for:\n",
    "\n",
    "soup.find_all(['h1', 'a', 'p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee4106",
   "metadata": {},
   "source": [
    "## Using Attributes\n",
    "\n",
    "We can also try to match the elements with relevant attributes. We can pass a dictionary to the attrs parameter of find_all with the desired attributes of the elements we’re looking for. If we want to find all of the elements with the \"banner\" class, for example, we could use the command:\n",
    "\n",
    "> soup.find_all(attrs={'class':'banner'})\n",
    "\n",
    "Or, we can specify multiple different attributes! What if we wanted a tag with a \"banner\" class and the id \"jumbotron\"?\n",
    "\n",
    "> soup.find_all(attrs={'class':'banner', 'id':'jumbotron'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41775b70",
   "metadata": {},
   "source": [
    "## Using A Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If our selection starts to get really complicated, we can separate out all of the logic that we’re using to choose a tag into its own function. Then, we can pass that function into .find_all()!\n",
    "\n",
    "def has_banner_class_and_hello_world(tag):\n",
    "    return tag.attr('class') == \"banner\" and tag.string == \"Hello world\"\n",
    " \n",
    "soup.find_all(has_banner_class_and_hello_world)\n",
    "\n",
    "#This command would find an element that looks like this:\n",
    "\n",
    "<div class=\"banner\">Hello world</div>\n",
    "\n",
    "#but not an element that looks like this:\n",
    "\n",
    "<div>Hello world</div>\n",
    "\n",
    "#Or this:\n",
    "\n",
    "<div class=\"banner\">What's up, world!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e58131",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1.\n",
    "Find all of the a elements on the page and store them in a list called turtle_links.\n",
    "\n",
    "2.\n",
    "Print turtle_links. Is this what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f22d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we import the libraries we are going to need\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then we get the html from the website and store it in a soup object\n",
    "webpage = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "soup = bs(webpage.content)\n",
    "\n",
    "#finally we find all the elements with tag A and save them in a list\n",
    "turtle_links = soup.find_all('a')\n",
    "print(turtle_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9198f9",
   "metadata": {},
   "source": [
    "## Select for CSS Selectors\n",
    "\n",
    "Another way to capture your desired elements with the soup object is to use CSS selectors. The .select() method will take in all of the CSS selectors you normally use in a .css file!\n",
    "\n",
    "<h1 class='results'>Search Results for: <span class='searchTerm'>Funfetti</span></h1>\n",
    "<div class='recipeLink'><a href=\"spaghetti.html\">Funfetti Spaghetti</a></div>\n",
    "<div class='recipeLink' id=\"selected\"><a href=\"lasagna.html\">Lasagna de Funfetti</a></div>\n",
    "<div class='recipeLink'><a href=\"cupcakes.html\">Funfetti Cupcakes</a></div>\n",
    "<div class='recipeLink'><a href=\"pie.html\">Pecan Funfetti Pie</a></div>\n",
    "\n",
    "If we wanted to select all of the elements that have the class 'recipeLink', we could use the command:\n",
    "\n",
    "> soup.select(\".recipeLink\")\n",
    "\n",
    "If we wanted to select the element that has the id 'selected', we could use the command:\n",
    "\n",
    "> soup.select(\"#selected\")\n",
    "\n",
    "Let’s say we wanted to loop through all of the links to these funfetti recipes that we found from our search.\n",
    "\n",
    "> for link in soup.select(\".recipeLink > a\"):\n",
    "      webpage = requests.get(link)\n",
    "      new_soup = BeautifulSoup(webpage)\n",
    "  \n",
    "This loop will go through each link in each .recipeLink div and create a soup object out of the webpage it links to. \n",
    "So, it would first make soup out of \n",
    "<a href=\"spaghetti.html\">Funfetti Spaghetti</a>, \n",
    "then <a href=\"lasagna.html\">Lasagna de Funfetti</a>, and so on.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1.\n",
    "We have taken the links you found in the last exercise and turned them into links we can follow by prepending a prefix to them.\n",
    "\n",
    "Now, we’re looping through each of the links on the home page and following them to learn more about each turtle!\n",
    "\n",
    "We are going to want to store information about each turtle in a dictionary called turtle_data.\n",
    "\n",
    "First, before the loop that goes through the turtle_links, create an empty dictionary called turtle_data.\n",
    "\n",
    "2.\n",
    "Now, let’s fill in some data from each turtle. Inside the loop that iterates through links, you’ll see that we’ve created a BeautifulSoup object out of each individual turtle’s page.\n",
    "\n",
    "You can click on a turtle in the web browser to see how an individual turtle’s page looks. What kind of information from this page might we want to store?\n",
    "\n",
    "3.\n",
    "If you used your Inspector tools to look at the turtle’s page, you might have seen that the turtle’s name is in a p tag with the class name.\n",
    "\n",
    "Inside the loop, use .select() to select the tags with class name. Store the first entry of that list in a variable called turtle_name.\n",
    "\n",
    "Add turtle_name to the dictionary as a key, and for now set the value of that key to an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4287f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we import the libraries we are going to need\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then we get the html content from the page and save it in a soup object\n",
    "webpage = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "soup = bs(webpage.content)\n",
    "\n",
    "#now we save the links of the turtles from tag 'a' into a list\n",
    "turtle_links = soup.find_all('a')\n",
    "\n",
    "#let's create a prefix of the html path to add to the links\n",
    "prefix = 'https://content.codecademy.com/courses/beautifulsoup/'\n",
    "\n",
    "#let's create an empty list to fill with the links\n",
    "links = []\n",
    "\n",
    "#let's go through all the 'a' tags and get the associated links:\n",
    "for a in turtle_links:\n",
    "    links.append(prefix + a['href'])\n",
    "    \n",
    "#we define the dictionary turtle_data to store the information for each turtle\n",
    "turtle_data = {}\n",
    "\n",
    "for link in links:\n",
    "    webpage = requests.get(link)\n",
    "    turtle = bs(webpage.content)\n",
    "    turtle_name = turtle.select('.name')[0]\n",
    "    turtle_data[turtle_name] = []\n",
    "    \n",
    "print(turtle_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3189f",
   "metadata": {},
   "source": [
    "## Reading Text\n",
    "\n",
    "When we use BeautifulSoup to select HTML elements, we often want to grab the text inside of the element, so that we can analyze it. We can use .get_text() to retrieve the text inside of whatever tag we want to call it on.\n",
    "\n",
    "<h1 class=\"results\">Search Results for: <span class='searchTerm'>Funfetti</span></h1>\n",
    "\n",
    "If this is the HTML that has been used to create the soup object, we can make the call:\n",
    "\n",
    "> soup.get_text()\n",
    "\n",
    "Which will return:\n",
    "\n",
    "    'Search Results for: Funfetti'\n",
    "\n",
    "Notice that this combined the text inside of the outer h1 tag with the text contained in the span tag inside of it! Using get_text(), it looks like both of these strings are part of just one longer string. If we wanted to separate out the texts from different tags, we could specify a separator character. This command would use a . character to separate:\n",
    "\n",
    "> soup.get_text('|')\n",
    "\n",
    "Now, the command returns:\n",
    "\n",
    "    'Search Results for: |Funfetti'\n",
    "    \n",
    " ### Instructions\n",
    " \n",
    "1.\n",
    "After the loop, print out turtle_data. We have been storing the names as the whole p tag containing the name.\n",
    "\n",
    "Instead, let’s call get_text() on the turtle_name element and store the result as the key of our dictionary instead.\n",
    "\n",
    "\n",
    "2.\n",
    "Instead of associating each turtle with an empty list, let’s have each turtle associated with a list of the stats that are available on their page.\n",
    "\n",
    "It looks like each piece of information is in a li element on the turtle’s page.\n",
    "\n",
    "Get the ul element on the page, and get all of the text in it, separated by a '|' character so that we can easily split out each attribute later.\n",
    "\n",
    "Store the resulting string in turtle_data[turtle_name] instead of storing an empty list there.\n",
    "\n",
    "\n",
    "3.\n",
    "When we store the list of info in each turtle_data[turtle_name], separate out each list element again by splitting on '|'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we install the libraries we are going to need\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then get get the html webpage and save the content in a soup object\n",
    "webpage = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "soup = bs(webpage.content)\n",
    "\n",
    "#now we save the links from each turtle, with tag 'A' into a list\n",
    "turtle_links = soup.find_all('a')\n",
    "\n",
    "#let's create a prefix of the html path to add to the links\n",
    "prefix = 'https://content.codecademy.com/courses/beautifulsoup/'\n",
    "\n",
    "#let's create an empty list to fill with the links\n",
    "links = []\n",
    "\n",
    "#let's go through all the 'a' tags and get the associated links:\n",
    "for a in turtle_links:\n",
    "    links.append(prefix + a['href'])\n",
    "    \n",
    "#we define the dictionary turtle_data to store the information for each turtle\n",
    "turtle_data = {}\n",
    "\n",
    "for link in links:\n",
    "    webpage = requests.get(link)\n",
    "    turtle = bs(webpage.content)\n",
    "    turtle_name = turtle.select('.name')[0].get_text()\n",
    "    stats = turtle.find('ul').get_text('|')\n",
    "    turtle_data[turtle_name] = stats.split('|')\n",
    "\n",
    "print(turtle_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0fc0b",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Amazing! Now you know the basics of how to use BeautifulSoup to turn websites into data. If you take our Data Visualization or Data Manipulation courses, you can see how you might analyze this data and find patterns!\n",
    "\n",
    "You now can see how far the rabbit hole goes by finding some interesting data you want to analyze on the web. But remember to be respectful to site owners if you test out your scraping chops on real sites.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1.\n",
    "Create a DataFrame out of the turtle_data dictionary you’ve created. Call it turtle_df.\n",
    "\n",
    "\n",
    "2.\n",
    "Wow! Now we have all of the turtles’ information in one DataFrame. But obviously, in just scraping this data and plopping it into Pandas, we’re left with a pretty messy DataFrame.\n",
    "\n",
    "There are newlines in the data, the column names are hidden in strings in the rows, and none of the numerical data is stored as a numerical type.\n",
    "\n",
    "It would be pretty hard to create any sort of analysis on this raw data. What if we wanted to make a histogram of the ages of turtles in the Shellter?\n",
    "\n",
    "This is where Data Cleaning and Regex comes in! Try to practice what you know about data cleaning to get turtles_df into a usable state. It’s up to you to decide what “usable” means to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76612085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we install the libraries we are going to need\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "#then get get the html webpage and save the content in a soup object\n",
    "webpage = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "soup = bs(webpage.content)\n",
    "\n",
    "#now we save the links from each turtle, with tag 'A' into a list\n",
    "turtle_links = soup.find_all('a')\n",
    "\n",
    "#let's create a prefix of the html path to add to the links\n",
    "prefix = 'https://content.codecademy.com/courses/beautifulsoup/'\n",
    "\n",
    "#let's create an empty list to fill with the links\n",
    "links = []\n",
    "\n",
    "#let's go through all the 'a' tags and get the associated links:\n",
    "for a in turtle_links:\n",
    "    links.append(prefix + a['href'])\n",
    "    \n",
    "#we define the dictionary turtle_data to store the information for each turtle\n",
    "turtle_data = {}\n",
    "\n",
    "for link in links:\n",
    "    webpage = requests.get(link)\n",
    "    turtle = bs(webpage.content)\n",
    "    turtle_name = turtle.select('.name')[0].get_text()\n",
    "    stats = turtle.find('ul').get_text('|')\n",
    "    turtle_data[turtle_name] = stats.split('|')\n",
    "    \n",
    "turtle_df = pd.DataFrame.from_dict(turtle_data, orient = 'index')\n",
    "print(turtle_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b1a6d",
   "metadata": {},
   "source": [
    "                                                .\n",
    "                                                .\n",
    "                                                .\n",
    "                                                .\n",
    "                                                .\n",
    "                                                ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fdb53",
   "metadata": {},
   "source": [
    "# SCRAPING PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153dbd2",
   "metadata": {},
   "source": [
    "## Chocolate Scraping with Beautiful Soup\n",
    "\n",
    "After eating chocolate bars your whole life, you’ve decided to go on a quest to find the greatest chocolate bar in the world.\n",
    "\n",
    "You’ve found a website that has over 1700 reviews of chocolate bars from all around the world. It’s displayed in the web browser on this page.\n",
    "\n",
    "The data is displayed in a table, instead of in a csv or json. Thankfully, we have the power of BeautifulSoup that will help us transform this webpage into a DataFrame that we can manipulate and analyze.\n",
    "\n",
    "The rating scale is from 1-5, as described in this review guide. A 1 is “unpleasant” chocolate, while a 5 is a bar that transcends “beyond the ordinary limits”.\n",
    "\n",
    "Some questions we thought about when we found this dataset were:\n",
    "\n",
    "-Where are the best cacao beans grown?\n",
    "-Which countries produce the highest-rated bars?\n",
    "-What’s the relationship between cacao solids percentage and rating?\n",
    "\n",
    "Can we find a way to answer these questions, or uncover more questions, using BeautifulSoup and Pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84356521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we install the pertinent libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ee695",
   "metadata": {},
   "source": [
    "### Make some Chocolate Soup\n",
    "\n",
    "1.\n",
    "Explore the webpage displayed in the browser. What elements could be useful to scrape here? Which elements do we not want to scrape?\n",
    "\n",
    "\n",
    "2.\n",
    "Let’s make a request to this site to get the raw HTML, which we can later turn into a BeautifulSoup object.\n",
    "\n",
    "The URL is:\n",
    "\n",
    "https://content.codecademy.com/courses/beautifulsoup/cacao/index.html\n",
    "\n",
    "You can pass this into the .get() method of the requests module to get the HTML.\n",
    "\n",
    "\n",
    "3.\n",
    "Create a BeautifulSoup object called soup to traverse this HTML.\n",
    "\n",
    "Use \"html.parser\" as the parser, and the content of the response you got from your request as the document.\n",
    "\n",
    "\n",
    "4.\n",
    "If you want, print out the soup object to explore the HTML.\n",
    "\n",
    "So many table rows! You’re probably very relieved that we don’t have to scrape this information by hand.\n",
    "\n",
    "\n",
    "5.\n",
    "How many terrible chocolate bars are out there? And how many earned a perfect 5? Let’s make a histogram of this data.\n",
    "\n",
    "The first thing to do is to put all of the ratings into a list.\n",
    "\n",
    "Use a command on the soup object to get all of the tags that contain the ratings.\n",
    "\n",
    "\n",
    "6.\n",
    "Create an empty list called ratings to store all the ratings in.\n",
    "\n",
    "\n",
    "7.\n",
    "Loop through the ratings tags and get the text contained in each one. Add it to the ratings list.\n",
    "\n",
    "As you do this, convert the rating to a float, so that the ratings list will be numerical. This should help with calculations later.\n",
    "\n",
    "\n",
    "8.\n",
    "Using Matplotlib, create a histogram of the ratings values:\n",
    "\n",
    "plt.hist(ratings)\n",
    "Remember to show the plot using plt.show()!\n",
    "\n",
    "Your plot will show up at localhost in the web browser. You will have to navigate away from the cacao ratings webpage to see it.\n",
    "\n",
    "\n",
    "#### Which chocolatier makes the best chocolate?\n",
    "\n",
    "9.\n",
    "We want to now find the 10 most highly rated chocolatiers. One way to do this is to make a DataFrame that has the chocolate companies in one column, and the ratings in another. Then, we can do a groupby to find the ones with the highest average rating.\n",
    "\n",
    "First, let’s find all the tags on the webpage that contain the company names.\n",
    "\n",
    "\n",
    "10.\n",
    "Just like we did with ratings, we now want to make an empty list to hold company names.\n",
    "\n",
    "\n",
    "11.\n",
    "Loop through the tags containing company names, and add the text from each tag to the list you just created.\n",
    "\n",
    "\n",
    "12.\n",
    "Create a DataFrame with a column “Company” corresponding to your companies list, and a column “Ratings” corresponding to your ratings list.\n",
    "\n",
    "\n",
    "13.\n",
    "Use .groupby to group your DataFrame by Company and take the average of the grouped ratings.\n",
    "\n",
    "Then, use the .nlargest command to get the 10 highest rated chocolate companies. Print them out.\n",
    "\n",
    "Look at the hint if you get stuck on this step!\n",
    "\n",
    "\n",
    "#### Is more cacao better?\n",
    "\n",
    "14.\n",
    "We want to see if the chocolate experts tend to rate chocolate bars with higher levels of cacoa to be better than those with lower levels of cacoa.\n",
    "\n",
    "It looks like the cocoa percentages are in the table under the Cocoa Percent column (note we are looking at cocoa not cocao!).\n",
    "\n",
    "Using the same methods you used in the last couple of tasks, create a list that contains all of the cocoa percentages. Store each percent as a float, after stripping off the % character.\n",
    "\n",
    "\n",
    "15.\n",
    "Add the cocoa percentages as a column called \"CocoaPercentage\" in the DataFrame that has companies and ratings in it.\n",
    "\n",
    "\n",
    "16.\n",
    "Make a scatterplot of ratings (your_df.Rating) vs percentage of cocoa (your_df.CocoaPercentage).\n",
    "\n",
    "You can do this in Matplotlib with these commands:\n",
    "\n",
    "plt.scatter(df.CocoaPercentage, df.Rating)\n",
    "plt.show()\n",
    "Call plt.clf() to clear the figure between showing your histogram and this scatterplot.\n",
    "\n",
    "Remember that your plots will show up at the address localhost in the web browser.\n",
    "\n",
    "\n",
    "17.\n",
    "Is there any correlation here? We can use some numpy commands to draw a line of best-fit over the scatterplot.\n",
    "\n",
    "Copy this code and paste it after you create the scatterplot, but before you call .show():\n",
    "\n",
    "z = np.polyfit(df.CocoaPercentage, df.Rating, 1)\n",
    "line_function = np.poly1d(z)\n",
    "plt.plot(df.CocoaPercentage, line_function(df.CocoaPercentage), \"r--\")\n",
    "\n",
    "\n",
    "#### Explore!\n",
    "\n",
    "18.\n",
    "We have explored a couple of the questions about chocolate that inspired us when we looked at this chocolate table.\n",
    "\n",
    "What other kinds of questions can you answer here? Try to use a combination of BeautifulSoup and Pandas to explore some more.\n",
    "\n",
    "For inspiration: Where are the best cocoa beans grown? Which countries produce the highest-rated bars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e5618dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f0a81b6f670>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#answering to task 1, we probably want to scrape the table and save it in a dictionary that we can covert into a dataframe\n",
    "#we don't want to scrape any other part of the body, like the photo or title, etc. it's not useful for our goal\n",
    "\n",
    "#now we start with task 2. Let's make a request to get the html from the page.\n",
    "chocolate_page = requests.get('https://content.codecademy.com/courses/beautifulsoup/cacao/index.html')\n",
    "\n",
    "#Task 3, create a soup object of the html content\n",
    "chocolate_soup = bs(chocolate_page.content, \"html.parser\")\n",
    "\n",
    "#Task 4, let's print the soup object\n",
    "#print(chocolate_soup.prettify)\n",
    "\n",
    "#Task 5, let's figure out the number of terrible chocolate bars, and the ones with a 5 rating\n",
    "#Let's get the ratings tags from the soup object\n",
    "table = chocolate_soup.select(\"table\")[1]\n",
    "ratings_tags = table.find_all('td', attrs = {\"class\":\"Rating\"})\n",
    "\n",
    "#Task 6, we create an empty list and loop through the tags to store all the text\n",
    "#Task 7,let's now loop through the ratings tags and get the text\n",
    "ratings = []\n",
    "for td in ratings_tags [1:]:\n",
    "    txt = float(td.get_text())\n",
    "    ratings.append(txt)\n",
    "\n",
    "#print(ratings)\n",
    "\n",
    "#Task 8, let's create now a histogram\n",
    "plt.hist(ratings)\n",
    "\n",
    "#Task 9, let's find out who makes the best chocolate\n",
    "#First we find the company tags\n",
    "company_tags = table.find_all(\"td\", attrs = {\"class\":\"Company\"})\n",
    "\n",
    "#Task 10, now we are going to create an empty list\n",
    "companies = []\n",
    "\n",
    "#Task 11, and loop through the tags to get the text\n",
    "for td in company_tags [1:]:\n",
    "  company = td.get_text()\n",
    "  companies.append(company)\n",
    "\n",
    "#print(companies)\n",
    "\n",
    "#Task 12, let's create the DataFrame\n",
    "df_ratings = pd.DataFrame({\"Companies\" : companies , \"Ratings\" : ratings})\n",
    "\n",
    "#Task 13, let's calculate the average rating per company using group_by\n",
    "df_ratings_average = df_ratings.groupby([\"Companies\"]).mean()\n",
    "df_ratings_average.nlargest(10, \"Ratings\")\n",
    "\n",
    "#Task 14, lets's create a list with the cocoa percentages\n",
    "cocoa_tags = table.find_all(\"td\", attrs = {\"class\":\"CocoaPercent\"})\n",
    "\n",
    "cocoa_percentage = []\n",
    "\n",
    "for td in cocoa_tags [1:]:\n",
    "    num = float(td.get_text().strip(\"%\"))\n",
    "    cocoa_percentage.append(num)\n",
    "    \n",
    "#Task 15, let's add this list to the data frame\n",
    "df_ratings.insert(2, \"CocoaPercentage\", cocoa_percentage, True)\n",
    "df_ratings.head()\n",
    "\n",
    "#Task 16, scatterplot of ratings vs cocoa%\n",
    "plt.clf()\n",
    "plt.scatter(df_ratings.CocoaPercentage, df_ratings.Ratings)\n",
    "\n",
    "#Task 17, let's study the correlation using numpy\n",
    "plt.clf()\n",
    "z = np.polyfit(df_ratings.CocoaPercentage, df_ratings.Ratings, 1)\n",
    "line_function = np.poly1d(z)\n",
    "plt.plot(df_ratings.CocoaPercentage, line_function(df_ratings.CocoaPercentage), \"r--\")\n",
    "plt.scatter(df_ratings.CocoaPercentage, df_ratings.Ratings)\n",
    "\n",
    "#Explore (to do)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110f49a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582daa9",
   "metadata": {},
   "source": [
    "#### CHECK THE BeautifulSoup library documentarion here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CHECK THE CHEATSEET FOR CODECADEMY BEAUTIFUL COURSE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
