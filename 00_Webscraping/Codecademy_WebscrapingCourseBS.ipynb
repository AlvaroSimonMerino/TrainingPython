{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4c2090",
   "metadata": {},
   "source": [
    "# Wescraping BS Codecademy COurse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a8a364",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea01c4",
   "metadata": {},
   "source": [
    "Before we get started, a quick note on prerequisites: This course requires knowledge of Python. Also some understanding of the Python library Pandas will be helpful later on in the lesson, but isn’t totally necessary. If you haven’t already, check out those courses before taking this one. Okay, let’s get scraping!\n",
    "\n",
    "In Data Science, we can do a lot of exciting work with the right dataset. Once we have interesting data, we can use Pandas or Matplotlib to analyze or visualize trends. But how do we get that data in the first place?\n",
    "\n",
    "If it’s provided to us in a well-organized csv or json file, we’re lucky! Most of the time, we need to go out and search for it ourselves.\n",
    "\n",
    "Often times you’ll find the perfect website that has all the data you need, but there’s no way to download it. This is where BeautifulSoup comes in handy to scrape the HTML. If we find the data we want to analyze online, we can use BeautifulSoup to grab it and turn it into a structure we can understand. This Python library, which takes its name from a song in Alice in Wonderland, allows us to easily and quickly take information from a website and put it into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5bd9e",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075bb40",
   "metadata": {},
   "source": [
    "1.We’ve used BeautifulSoup to take the turtle data from the Shellter website in the browser and put it into a DataFrame.\n",
    "\n",
    "Explore the website a bit. Then, print the DataFrame turtles to see how this data is organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25bca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import turtles\n",
    "print(turtles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39095e3e",
   "metadata": {},
   "source": [
    "                            0  ...                             4\n",
    "Aesop        AGE: 7 Years Old  ...    SOURCE: found in Lake Erie\n",
    "Caesar       AGE: 2 Years Old  ...      SOURCE: hatched in house\n",
    "Sulla         AGE: 1 Year Old  ...    SOURCE: found in Lake Erie\n",
    "Spyro        AGE: 6 Years Old  ...      SOURCE: hatched in house\n",
    "Zelda        AGE: 3 Years Old  ...  SOURCE: surrendered by owner\n",
    "Bandicoot    AGE: 2 Years Old  ...      SOURCE: hatched in house\n",
    "Hal           AGE: 1 Year Old  ...  SOURCE: surrendered by owner\n",
    "Mock        AGE: 10 Years Old  ...  SOURCE: surrendered by owner\n",
    "Sparrow    AGE: 1.5 Years Old  ...    SOURCE: found in Lake Erie\n",
    "\n",
    "[9 rows x 5 columns]\n",
    " \n",
    "1/11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f428d4c",
   "metadata": {},
   "source": [
    "## Rules of Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725fc0f",
   "metadata": {},
   "source": [
    "When we scrape websites, we have to make sure we are following some guidelines so that we are treating the websites and their owners with respect.\n",
    "\n",
    "Always check a website’s Terms and Conditions before scraping. Read the statement on the legal use of data. Usually, the data you scrape should not be used for commercial purposes.\n",
    "\n",
    "Do not spam the website with a ton of requests. A large number of requests can break a website that is unprepared for that level of traffic. As a general rule of good practice, make one request to one webpage per second.\n",
    "\n",
    "If the layout of the website changes, you will have to change your scraping code to follow the new structure of the site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c0639",
   "metadata": {},
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941775f",
   "metadata": {},
   "source": [
    "In order to get the HTML of the website, we need to make a request to get the content of the webpage. To learn more about requests in a general sense, you can check out this https://www.codecademy.com/articles/http-requests.\n",
    "\n",
    "Python has a *requests* library that makes getting content really easy. All we have to do is import the library, and then feed in the URL we want to *GET*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d016314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "webpage = requests.get('https://www.codecademy.com/articles/http-requests')\n",
    "print(webpage.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f165006",
   "metadata": {},
   "source": [
    "This code will print out the HTML of the page.\n",
    "\n",
    "We don’t want to unleash a bunch of requests on any one website in this lesson, so for the rest of this lesson we will be scraping a local HTML file and pretending it’s an HTML file hosted online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a180c",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.\n",
    "Import the requests library.\n",
    "\n",
    "\n",
    "2.\n",
    "Make a GET request to the URL containing the turtle adoption website:\n",
    "\n",
    "https://content.codecademy.com/courses/beautifulsoup/shellter.html\n",
    "\n",
    "Store the result of your request in a variable called webpage_response.\n",
    "\n",
    "\n",
    "3.\n",
    "Store the content of the response in a variable called webpage by using .content.\n",
    "\n",
    "Print webpage out to see the content of this HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79afc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #import the requests library\n",
    "webpage_response = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html') #make a get request\n",
    "webpage = webpage_response.content #store the content of the HTML\n",
    "print(webpage) #print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89262d0c",
   "metadata": {},
   "source": [
    "## The BeautifulSoup Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3820e",
   "metadata": {},
   "source": [
    "When we printed out all of that HTML from our request, it seemed pretty long and messy. How could we pull out the relevant information from that long string?\n",
    "\n",
    "BeautifulSoup is a Python library that makes it easy for us to traverse an HTML page and pull out the parts we’re interested in. We can import it by using the line:\n",
    "\n",
    "*from bs4 import BeautifulSoup*\n",
    "\n",
    "Then, all we have to do is convert the HTML document to a BeautifulSoup object!\n",
    "\n",
    "If this is our HTML file, rainbow.html:\n",
    "\n",
    "<body>\n",
    "  <div>red</div>\n",
    "  <div>orange</div>\n",
    "  <div>yellow</div>\n",
    "  <div>green</div>\n",
    "  <div>blue</div>\n",
    "  <div>indigo</div>\n",
    "  <div>violet</div>\n",
    "</body>\n",
    "\n",
    "*soup = BeautifulSoup(\"rainbow.html\", \"html.parser\")*\n",
    "\n",
    "\"html.parser\" is one option for parsers we could use. There are other options, like \"lxml\" and \"html5lib\" that have different advantages and disadvantages, but for our purposes we will be using \"html.parser\" throughout.\n",
    "\n",
    "With the requests skills we just learned, we can use a website hosted online as that HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "webpage = requests.get(\"http://rainbow.com/rainbow.html\", \"html.parser\")\n",
    "soup = bs(webpage.content)\n",
    "\n",
    "#When we use BeautifulSoup in combination with pandas, we can turn websites into DataFrames \n",
    "#that are easy to manipulate and gain insights from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32882236",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cfc8b",
   "metadata": {},
   "source": [
    "1.\n",
    "Import the BeautifulSoup package.\n",
    "\n",
    "\n",
    "2.\n",
    "Create a BeautifulSoup object out of the webpage content and call it soup. Use \"html.parser\" as the parser.\n",
    "\n",
    "Print out soup! Look at how it contains all of the HTML of the page! We will learn how to traverse this content and find what we need in the next exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747deda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import the needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then we use *get* from *requests* to pull the website html content in *wepage_response*\n",
    "webpage_response = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "#we store that content in the variable webpage\n",
    "webpage = webpage_response.content\n",
    "#then we create a soup object to store the parsed html of the page\n",
    "soup = bs(webpage,'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f3e7f",
   "metadata": {},
   "source": [
    "## Object Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfdb3c",
   "metadata": {},
   "source": [
    "BeautifulSoup breaks the HTML page into several types of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73f97e",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de022368",
   "metadata": {},
   "source": [
    "A Tag corresponds to an HTML Tag in the original document. These lines of code:\n",
    "\n",
    "soup = BeautifulSoup('<div id=\"example\">An example div</div><p>An example p tag</p>')\n",
    "print(soup.div)\n",
    "\n",
    "Would produce output that looks like:\n",
    "\n",
    "<div id=\"example\">An example div</div>\n",
    "\n",
    "Accessing a tag from the BeautifulSoup object in this way will get the first tag of that type on the page.\n",
    "\n",
    "You can get the name of the tag using .name and a dictionary representing the attributes of the tag using .attrs:\n",
    "\n",
    "print(soup.div.name)\n",
    "print(soup.div.attrs)\n",
    "\n",
    "div\n",
    "{'id': 'example'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we try the code explained in the previous paragraph\n",
    "soup = bs('<div id=\"example\">An example div</div><p>An example p tag</p>')\n",
    "print(soup.div)\n",
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a1c34",
   "metadata": {},
   "source": [
    "Accessing a tag from the BeautifulSoup object in this way will get the first tag of that type on the page.\n",
    "\n",
    "You can get the name of the tag using .name and a dictionary representing the attributes of the tag using .attrs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.div.name) #div\n",
    "print(soup.div.attrs) #{'id': 'example'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa0d52",
   "metadata": {},
   "source": [
    "## NavigableStrings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51efe2",
   "metadata": {},
   "source": [
    "NavigableStrings are the pieces of text that are in the HTML tags on the page. You can get the string inside of the tag by calling .string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.div.string) #An example div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4eeac2",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1.\n",
    "Print out the first p tag on the shellter.html page.\n",
    "\n",
    "2.\n",
    "Print out the string associated with the first p tag on the shellter.html page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6725fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import the needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#then we request to access a webpage content, and save it\n",
    "webpage_response = requests.get('https://content.codecademy.com/courses/beautifulsoup/shellter.html')\n",
    "webpage = webpage_response.content\n",
    "\n",
    "#then we create the bs object\n",
    "soup = bs(webpage, 'html.parser')\n",
    "\n",
    "#now we solve exercise 1\n",
    "print(soup.p) #output --> <p class=\"text\">Click to learn more about each turtle</p>\n",
    "\n",
    "#and we solve exercise 2\n",
    "print(soup.p.string) #output --> Click to learn more about each turtle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the webpage using parser\n",
    "soup = bs(webpage, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the website without parser *barely any difference*\n",
    "soup2 =bs(webpage)\n",
    "print(soup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e058dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the website using prettify() organizes the html in hierarchy\n",
    "soup3 =bs(webpage)\n",
    "print(soup3.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21117895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e0084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3ddf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de1ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18200f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf30505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9c60626",
   "metadata": {},
   "source": [
    "#### CHECK THE BeautifulSoup library documentarion here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593604e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CHECK THE CHEATSEET FOR CODECADEMY BEAUTIFUL COURSE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
